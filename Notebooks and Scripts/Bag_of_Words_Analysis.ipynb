{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bag of Words Analysis",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXL5dCri-3LZ"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvuk5ggf--18",
        "outputId": "70583dce-4479-4f76-90de-fbcbc831b5da"
      },
      "source": [
        "!git clone https://github.com/tdeme/NLP-Political-Bias.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NLP-Political-Bias'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 8 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (8/8), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly__0r1s---y"
      },
      "source": [
        "left_tweets_df = pd.read_csv('/content/NLP-Political-Bias/Rleft_tweets.csv')\n",
        "right_tweets_df = pd.read_csv('/content/NLP-Political-Bias/Rright_tweets.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkiTgnOK_SWt",
        "outputId": "11676409-cf00-4899-9a80-ccdb77176290"
      },
      "source": [
        "print(len(left_tweets_df))\n",
        "print(len(right_tweets_df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49532\n",
            "40105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x3DluVM_ZDI",
        "outputId": "d4a4bbb6-a71f-41ea-d35f-ce1785c37b39"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk_stopwords = stopwords.words('english')\n",
        "for stopword in ('http','https','co'):\n",
        "  nltk_stopwords.append(stopword)\n",
        "\n",
        "left_tf_vectorizer = CountVectorizer(max_df=0.8, min_df=25, ngram_range=(2,3), binary=True, stop_words=nltk_stopwords)\n",
        "right_tf_vectorizer = CountVectorizer(max_df=0.8, min_df=25, ngram_range=(2,3), binary=True, stop_words=nltk_stopwords)\n",
        "\n",
        "left_term_freqs = left_tf_vectorizer.fit_transform(left_tweets_df['tweet'].values.astype('U'))\n",
        "right_term_freqs = right_tf_vectorizer.fit_transform(right_tweets_df['tweet'].values.astype('U'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k22d0RN_diD",
        "outputId": "412d3710-1bb8-4e9a-d783-3b4b1532dbda"
      },
      "source": [
        "#Make dataframes for the term frequencies for left and right\n",
        "\n",
        "left_phrases_df = pd.DataFrame(data=left_tf_vectorizer.get_feature_names(),columns=['phrase'])\n",
        "left_phrases_df['total_occurences'] = left_term_freqs.sum(axis=0).T\n",
        "left_phrases_df.sort_values(by='total_occurences',ascending=False).head(50).to_csv('left_top_50.csv', index=False)\n",
        "\n",
        "right_phrases_df = pd.DataFrame(data=right_tf_vectorizer.get_feature_names(),columns=['phrase'])\n",
        "right_phrases_df['total_occurences'] = right_term_freqs.sum(axis=0).T\n",
        "right_phrases_df.sort_values(by='total_occurences',ascending=False).head(50).to_csv('right_top_50.csv', index=False)\n",
        "\n",
        "print(left_phrases_df)\n",
        "print(right_phrases_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                           phrase  total_occurences\n",
            "0                   000 americans               105\n",
            "1                    000 children                36\n",
            "2                    000 dreamers                25\n",
            "3                     000 federal                38\n",
            "4                       000 month                27\n",
            "...                           ...               ...\n",
            "2310  youtube video congresswoman                26\n",
            "2311                      yrs ago                28\n",
            "2312               zero tolerance                43\n",
            "2313        zero tolerance policy                29\n",
            "2314                     zip code                31\n",
            "\n",
            "[2315 rows x 2 columns]\n",
            "                 phrase  total_occurences\n",
            "0              000 jobs                45\n",
            "1               000 new                43\n",
            "2          000 new jobs                29\n",
            "3               100 000                30\n",
            "4              100 days                26\n",
            "...                 ...               ...\n",
            "1129         years come                29\n",
            "1130      years service                25\n",
            "1131  yesterday discuss                38\n",
            "1132        yet another                65\n",
            "1133       young people                26\n",
            "\n",
            "[1134 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
